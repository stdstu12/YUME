# webapp_single_gpu.py
# Flask ç‰ˆï¼šé•¿è§†é¢‘ç”Ÿæˆï¼ˆå•å›¾ i2v é¦–æ®µ + ç»­å¸§ï¼‰ï¼Œå•å¡ã€å…¨ç¨‹ BF16ï¼Œ
# 4 æ¨¡å‹ï¼štransformer & vae (GPU å¸¸é©»)ï¼Œtext_encoder & caption_model (CPU å¸¸é©»ï¼Œä¸´æ—¶ä¸Š GPU)
# é‡‡æ ·é€»è¾‘ä¸ sample_one å¯¹é½ï¼šä»…æ›´æ–°å°¾éƒ¨ latent_frame_zero å¸§ï¼Œé€æ®µæ‹¼æ¥è¾“å‡º

from import_shim import ensure_packages, WAN_CONFIGS
ensure_packages()

import os
import sys
import time
import platform
import socket
import traceback
from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, Any

import logging
from logging.handlers import RotatingFileHandler

from flask import Flask, jsonify, request, send_from_directory, Response
try:
    from flask_cors import CORS  # å¯é€‰
    _HAS_CORS = True
except Exception:
    _HAS_CORS = False

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
import numpy as np
from PIL import Image
from diffusers.utils import export_to_video

# ----------------------------- Logging setup -----------------------------
def setup_logging(app_name: str = "webapp", level=logging.INFO):
    log_dir = os.path.abspath("logs")
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"{app_name}_{time.strftime('%Y%m%d_%H%M%S')}.log")

    logger = logging.getLogger(app_name)
    logger.setLevel(level)
    logger.propagate = False
    for h in list(logger.handlers):
        logger.removeHandler(h)

    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s",
                            datefmt="%Y-%m-%d %H:%M:%S")

    fh = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=3, encoding="utf-8")
    fh.setFormatter(fmt)
    fh.setLevel(level)
    logger.addHandler(fh)

    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(fmt)
    ch.setLevel(logging.INFO)
    logger.addHandler(ch)

    def _excepthook(exc_type, exc, tb):
        logger.critical("UNCAUGHT EXCEPTION", exc_info=(exc_type, exc, tb))
        try:
            sys.__excepthook__(exc_type, exc, tb)
        except Exception:
            pass
    sys.excepthook = _excepthook

    try:
        import transformers, diffusers
        cuda_ok = torch.cuda.is_available()
        dev = torch.cuda.get_device_name(0) if cuda_ok else "CPU"
        logger.info("==== Runtime Env ====")
        logger.info("Python: %s", sys.version.replace("\n", " "))
        logger.info("OS: %s %s", platform.system(), platform.version())
        logger.info("torch: %s (cuda=%s) | transformers: %s | diffusers: %s",
                    torch.__version__, cuda_ok,
                    getattr(transformers, "__version__", "?"),
                    getattr(diffusers, "__version__", "?"))
        logger.info("Device: %s", dev)
    except Exception as e:
        logger.warning("Env probe failed: %s", e)

    return logger, log_file

LOGGER, LOG_PATH = setup_logging("webapp")
LOGGER.info("Log file: %s", LOG_PATH)

# ------------------------- Paths & runtime options -----------------------
CKPT_DIR = "./Wan2.2-TI2V-5B"              # Wan checkpoint dir
INTERNVL_PATH = "./InternVL3-2B-Instruct"  # InternVL dir
DEVICE_ID = 0                               # single GPU index
DTYPE = torch.bfloat16                      # å…¨ç¨‹ BF16
OUTPUT_DIR = os.path.abspath("outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ----------------------------- Small utils ------------------------------
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision("high")  # è®© SDPA/MatMul åœ¨éœ€è¦æ—¶èµ° TF32

def build_transform(input_size: int):
    return T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=T.InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

def get_sampling_sigmas(steps: int, shift: float):
    sigma = np.linspace(1, 0, steps + 1)[:steps]
    return (shift * sigma / (1 + (shift - 1) * sigma))

@torch.inference_mode()
def _postprocess_video(video: torch.Tensor, fps: int, out_path: str):
    # video: (C,F,H,W) in [-1,1]
    v = (video.clamp(-1,1).add(1).div(2))
    v = (v * 255).byte().cpu().numpy()       # (C,F,H,W)
    v = np.transpose(v, (1,2,3,0))           # (F,H,W,C)
    frames = [Image.fromarray(f) for f in v]
    export_to_video(frames, out_path, fps=fps)

def create_video_from_image(image_path: str, total_frames: int = 33, H1: int = 704, W1: int = 1280):
    """
    ä»å•å¼ å›¾ç‰‡åˆ›å»º (F=total_frames, C, H1, W1) çš„è§†é¢‘å¼ é‡ï¼š
    - ç¬¬ 0 å¸§æ”¾ç½®è¯¥å›¾ï¼ˆresize åˆ° H1xW1ï¼Œå¹¶åš [-1,1] å½’ä¸€åŒ–ï¼‰
    - å…¶ä»–å¸§ä¸º 0ï¼ˆåç»­é‡‡æ ·ä¼šåœ¨å°¾æ®µæ³¨å…¥/æ›´æ–°ï¼‰
    è¿”å›: (video(F,C,H,W), base_name, image_path)
    """
    if not os.path.isfile(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")
    img = Image.open(image_path).convert('RGB')
    arr = np.array(img)
    ten = torch.from_numpy(arr).permute(2,0,1).float() / 255.0  # (C,H,W)
    C,H,W = ten.shape
    vid = torch.zeros(C, total_frames, H1, W1)
    resized = F.interpolate(ten.unsqueeze(0), size=(H1,W1), mode='bilinear', align_corners=False)[0]
    vid[:,0] = (resized - 0.5) * 2
    base = os.path.splitext(os.path.basename(image_path))[0]
    return vid.permute(1,0,2,3), base, image_path  # (F,C,H,W)

# ----------------------------- Global state ------------------------------
@dataclass
class Models:
    device: Optional[torch.device] = None
    # Wan stack
    wan_i2v: Optional[object] = None
    transformer: Optional[nn.Module] = None
    vae: Optional[object] = None
    text_encoder: Optional[object] = None  # T5 (inside wan_i2v, kept CPU by default)
    # Caption
    caption_model: Optional[object] = None
    tokenizer: Optional[object] = None

MODELS = Models()
WAN_READY = False
CAP_READY = False

# é•¿è§†é¢‘ä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆå¯ç»­å¸§ï¼‰
LAST: Dict[str, Any] = {
    "last_model_input_latent": None,  # (C,F,H,W) latent
    "last_model_input_de": None,      # (C,F,H,W) pixel-space [-1,1]
    "frame_total": 0,
    "last_video_path": None,
    "last_prompt": "",
}

def _ensure_device():
    LOGGER.info("[device] checking CUDAâ€¦")
    if not torch.cuda.is_available():
        LOGGER.error("[device] CUDA not available.")
        raise RuntimeError("CUDA ä¸å¯ç”¨ï¼ŒWanTI2V éœ€è¦ GPUã€‚")
    torch.cuda.set_device(DEVICE_ID)
    dev_name = torch.cuda.get_device_name(DEVICE_ID)
    LOGGER.info("[device] using cuda:%d - %s", DEVICE_ID, dev_name)
    MODELS.device = torch.device(f"cuda:{DEVICE_ID}")
    torch.backends.cuda.matmul.allow_tf32 = True

def _trace_text(e: Exception) -> str:
    et = type(e).__name__
    return f"{et}: {e}\n\n" + traceback.format_exc()

# ---------- (ä¿ç•™) å¯èƒ½ç”¨åˆ°çš„ patch-embedding æ”¾å¤§ ----------
def upsample_conv3d_weights_auto(conv_small: nn.Conv3d, size: Tuple[int,int,int], device, dtype):
    OC, IC, _, _, _ = conv_small.weight.shape
    with torch.no_grad():
        w = F.interpolate(conv_small.weight.data.to(dtype=dtype, device=device),
                          size=size, mode='trilinear', align_corners=False)
        big = nn.Conv3d(in_channels=IC, out_channels=OC,
                        kernel_size=size, stride=size, padding=0,
                        dtype=dtype, device=device)
        big.weight.copy_(w)
        if conv_small.bias is not None:
            big.bias = nn.Parameter(conv_small.bias.data.to(dtype=dtype, device=device).clone())
        else:
            big.bias = None
    return big

# ------------------------ On-demand loaders (BF16) -----------------------
@torch.inference_mode()
def load_wan() -> str:
    global WAN_READY
    LOGGER.info("[load_wan] start (BF16, DEVICE_ID=%s)", DEVICE_ID)
    t0 = time.perf_counter()
    if WAN_READY:
        LOGGER.info("[load_wan] already loaded.")
        return "âœ… Wan å·²åŠ è½½ï¼ˆBF16ï¼‰"

    _ensure_device()
    import importlib
    _wan23 = importlib.import_module("wan23")

    cfg = WAN_CONFIGS["ti2v-5B"]
    wan_i2v = _wan23.WanTI2V(config=cfg, checkpoint_dir=CKPT_DIR, device_id=DEVICE_ID)
    transformer = wan_i2v.model
    vae = wan_i2v.vae
    text_encoder = wan_i2v.text_encoder  # T5 wrapperï¼ˆåç»­å§‹ç»ˆå¸¸é©» CPUï¼‰

    # transformer & vae å¸¸é©» GPU + BF16
    transformer = transformer.to(device=MODELS.device, dtype=DTYPE).eval()
    try:
        for p in vae.model.parameters():
            p.data = p.data.to(DTYPE)
        vae.model.to(device=MODELS.device)
    except Exception:
        vae.model.to(device=MODELS.device)

    # sideblock + mask_tokenï¼ˆä¸æ ·ä¾‹ä¸€è‡´ï¼‰
    from wan23.modules.model import WanAttentionBlock
    transformer.sideblock = WanAttentionBlock(
        transformer.dim, transformer.ffn_dim, transformer.num_heads,
        transformer.window_size, transformer.qk_norm, transformer.cross_attn_norm,
        transformer.eps
    ).to(device=MODELS.device, dtype=DTYPE)
    transformer.mask_token = nn.Parameter(torch.zeros(1,1,transformer.dim, device=MODELS.device, dtype=DTYPE))
    nn.init.normal_(transformer.mask_token, std=.02)
    transformer.eval()

    # T5 å¸¸é©» CPU
    try:
        text_encoder.model.cpu()
    except Exception:
        pass

    MODELS.wan_i2v = wan_i2v
    MODELS.transformer = transformer
    MODELS.vae = vae
    MODELS.text_encoder = text_encoder
    WAN_READY = True

    dt = time.perf_counter() - t0
    LOGGER.info("[load_wan] OK in %.2fs", dt)
    return f"âœ… Wan å·²åŠ è½½ï¼ˆBF16ï¼‰  ç”¨æ—¶ {dt:.1f}s"

@torch.inference_mode()
def load_caption_model() -> str:
    global CAP_READY
    LOGGER.info("[load_caption_model] start (BF16)")
    t0 = time.perf_counter()
    if CAP_READY:
        LOGGER.info("[load_caption_model] already loaded.")
        return "âœ… InternVL å·²åŠ è½½ï¼ˆBF16ï¼‰"

    from transformers import AutoModel, AutoTokenizer
    caption_model = AutoModel.from_pretrained(
        INTERNVL_PATH,
        torch_dtype=DTYPE,
        low_cpu_mem_usage=True,
        use_flash_attn=True,
        trust_remote_code=True
    ).eval()  # å…ˆæ”¾ CPU
    tokenizer = AutoTokenizer.from_pretrained(INTERNVL_PATH, trust_remote_code=True, use_fast=False)

    MODELS.caption_model = caption_model.cpu()
    MODELS.tokenizer = tokenizer
    CAP_READY = True

    dt = time.perf_counter() - t0
    LOGGER.info("[load_caption_model] OK in %.2fs", dt)
    return f"âœ… InternVL å·²åŠ è½½ï¼ˆBF16ï¼‰  ç”¨æ—¶ {dt:.1f}s"

# -------------------- Prompt ç²¾ç‚¼ï¼ˆä¸´æ—¶ä¸Š GPUï¼Œç”¨å®Œå› CPUï¼‰ --------------------
@torch.inference_mode()
def refine_prompt_from_image(image_path: str, user_prompt: str) -> str:
    if not CAP_READY or MODELS.caption_model is None:
        return user_prompt
    try:
        def dynamic_preprocess(image: Image.Image, min_num=1, max_num=12, image_size=448, use_thumbnail=True):
            orig_width, orig_height = image.size
            aspect_ratio = orig_width / orig_height
            target = set( (i, j)
                          for n in range(min_num, max_num + 1)
                          for i in range(1, n + 1)
                          for j in range(1, n + 1)
                          if i * j <= max_num and i * j >= min_num )
            target = sorted(target, key=lambda x: x[0]*x[1])

            best = (1,1); best_diff = 1e9
            for r in target:
                ar = r[0]/r[1]
                d = abs(aspect_ratio - ar)
                if d < best_diff: best_diff, best = d, r
            tw, th = best[0]*image_size, best[1]*image_size
            blocks = best[0]*best[1]

            resized = image.resize((tw, th))
            imgs = []
            for i in range(blocks):
                box = ((i % (tw//image_size))*image_size,
                       (i // (tw//image_size))*image_size,
                       ((i % (tw//image_size))+1)*image_size,
                       ((i // (tw//image_size))+1)*image_size)
                imgs.append(resized.crop(box))
            if use_thumbnail and len(imgs)!=1:
                imgs.append(image.resize((image_size,image_size)))
            return imgs

        tr = build_transform(448)
        img = Image.open(image_path).convert('RGB')
        tiles = dynamic_preprocess(img, image_size=448, use_thumbnail=True, max_num=12)
        px = torch.stack([tr(im) for im in tiles])

        caption_model = MODELS.caption_model.to(MODELS.device, dtype=DTYPE)
        px = px.to(MODELS.device, dtype=DTYPE)
        question = (f"<image>\nWe want to generate a video using this prompt: \"{user_prompt}\". "
                    "Please refine it for this image (<image>). Keep it one paragraph.")
        gen_cfg = dict(max_new_tokens=512, do_sample=True)
        out = caption_model.chat(MODELS.tokenizer, px, question, gen_cfg)
        MODELS.caption_model.cpu()
        return out or user_prompt
    except Exception as e:
        LOGGER.exception("[caption] refine failed: %s", e)
        try:
            MODELS.caption_model.cpu()
        except Exception:
            pass
        return user_prompt

# -------------------------- é•¿è§†é¢‘ç”Ÿæˆ ---------------------
@dataclass
class LongGenArgs:
    prompt: str
    jpg_path: Optional[str]
    output_dir: str
    fps: int
    sample_steps: int
    sample_num: int
    frame_zero: int
    latent_frame_zero: int
    shift: float
    seed: int
    continue_from_last: bool
    refine_from_image: bool
    caption_path: Optional[str]
    mode: str  # Added mode for I2V or T2V

def _to_bf16(x):
    if isinstance(x, torch.Tensor):
        return x.to(device=MODELS.device, dtype=DTYPE)
    if isinstance(x, (list, tuple)):
        return type(x)(_to_bf16(t) for t in x)
    return x

def tiled_decode_overlap(vae, latents: torch.Tensor, n_tiles: int = 5, 
                         image_overlap_size: int = 32, latent_frame_zero=None) -> torch.Tensor:
    """
    ç²¾ç¡®åŒ¹é…è¾“å‡ºå®½åº¦çš„åˆ†å—è§£ç å‡½æ•°
    
    å‚æ•°:
        vae: VAE æ¨¡å‹
        latents: è¾“å…¥ latent å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)
        n_tiles: åˆ†å—æ•°é‡
        image_overlap_size: å›¾åƒç©ºé—´çš„é‡å å¤§å°ï¼ˆåƒç´ ï¼‰
        latent_frame_zero: é€‰æ‹©æ—¶é—´ç»´åº¦çš„å‚æ•°
    
    è¿”å›:
        è§£ç åçš„å›¾åƒï¼Œå®½åº¦ä¸è¾“å…¥ latent ç²¾ç¡®åŒ¹é…
    """
    # è·å– latent å°ºå¯¸
    b, c, latents_h, latents_w = latents.shape
    
    # VAE ä¸Šé‡‡æ ·å› å­ï¼ˆæ ¹æ®æ‚¨çš„VAEè®¾ç½®ä¸º16ï¼‰
    scale_factor = 16
    
    # è®¡ç®—æœŸæœ›çš„è¾“å‡ºå®½åº¦
    expected_width = latents_w * scale_factor
    
    print(f"Latentå®½åº¦: {latents_w}, æœŸæœ›è¾“å‡ºå®½åº¦: {expected_width}")
    
    # è®¡ç®— latent ç©ºé—´çš„é‡å å¤§å°
    latent_overlap = max(1, image_overlap_size // scale_factor)
    print(f"Latentç©ºé—´é‡å å¤§å°: {latent_overlap}")
    
    # è®¡ç®—æ¯ä¸ªåˆ†å—çš„åŸºæœ¬å®½åº¦ï¼ˆlatent ç©ºé—´ï¼‰
    base_w = latents_w // n_tiles
    remainder = latents_w % n_tiles
    
    # åˆ†é…å®½åº¦ï¼Œè€ƒè™‘ä½™æ•°
    tile_widths = [base_w + 1 if i < remainder else base_w for i in range(n_tiles)]
    print(f"å„åˆ†å—å®½åº¦: {tile_widths}")
    
    # è®¡ç®—æ¯ä¸ªåˆ†å—çš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
    starts = []
    ends = []
    current = 0
    for i in range(n_tiles):
        # èµ·å§‹ä½ç½®
        start = current
        # ç»“æŸä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
        end = current + tile_widths[i]
        
        # ä¸ºé™¤ç¬¬ä¸€ä¸ªå¤–çš„æ‰€æœ‰åˆ†å—æ·»åŠ å‘å‰é‡å 
        if i > 0:
            start -= latent_overlap
            
        # ä¸ºé™¤æœ€åä¸€ä¸ªå¤–çš„æ‰€æœ‰åˆ†å—æ·»åŠ å‘åé‡å 
        if i < n_tiles - 1:
            end += latent_overlap
            
        start = max(start, 0)
        end = min(end, latents_w)
        
        starts.append(start)
        ends.append(end)
        current += tile_widths[i]
    
    print(f"åˆ†å—èµ·å§‹ä½ç½®: {starts}")
    print(f"åˆ†å—ç»“æŸä½ç½®: {ends}")
    
    # è§£ç æ¯ä¸ªåˆ†å—
    images = []
    for i in range(n_tiles):
        start = starts[i]
        end = ends[i]
        
        # æå– latent åˆ†å—
        if latent_frame_zero is not None:
            latent_chunk = latents[:, -latent_frame_zero:, :, start:end]
        else:
            latent_chunk = latents[:, :, :, start:end]
            
        print(f"åˆ†å— {i}: latentå°ºå¯¸ {latent_chunk.shape}")
        
        # è§£ç 
        with torch.no_grad():
            image_chunk = vae.decode([latent_chunk])[0]
        print(f"åˆ†å— {i}: è§£ç åå›¾åƒå°ºå¯¸ {image_chunk.shape}")
        images.append(image_chunk)
        
        # ç«‹å³é‡Šæ”¾æ˜¾å­˜
        del latent_chunk
        torch.cuda.empty_cache()
    
    # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„ç»“æœå¼ é‡
    result_height = images[0].shape[2]
    result = torch.zeros(images[0].shape[0], images[0].shape[1], result_height, expected_width, 
                        device=images[0].device, dtype=images[0].dtype)
    
    # åˆ›å»ºæ··åˆæƒé‡æ©ç 
    blend_mask = torch.zeros(result_height, expected_width, device=result.device)
    
    # è®¡ç®—æ¯ä¸ªåˆ†å—åœ¨ç»“æœä¸­çš„ä½ç½®
    positions = []
    for i in range(n_tiles):
        # è®¡ç®—è¿™ä¸ªåˆ†å—åœ¨ç»“æœä¸­çš„èµ·å§‹ä½ç½®
        start_pos = starts[i] * scale_factor
        
        # è®¡ç®—è¿™ä¸ªåˆ†å—åœ¨ç»“æœä¸­çš„ç»“æŸä½ç½®
        end_pos = ends[i] * scale_factor
        end_pos = min(end_pos, expected_width)  # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
        
        positions.append((start_pos, end_pos))
    
    print(f"å„åˆ†å—åœ¨ç»“æœä¸­çš„ä½ç½®: {positions}")
    
    # å¯¹æ¯ä¸ªåˆ†å—è¿›è¡ŒåŠ æƒæ··åˆ
    for i, (start_pos, end_pos) in enumerate(positions):
        image_chunk = images[i]
        chunk_width = image_chunk.shape[3]
        result_width_this_chunk = end_pos - start_pos
        
        print(f"åˆ†å— {i}: ç»“æœä½ç½® {start_pos}-{end_pos}, åˆ†å—å®½åº¦ {chunk_width}, éœ€è¦å®½åº¦ {result_width_this_chunk}")
        
        # åˆ›å»ºè¿™ä¸ªåˆ†å—çš„æƒé‡æ©ç ï¼ˆåªé’ˆå¯¹åˆ†å—å¯¹åº”çš„åŒºåŸŸï¼‰
        chunk_mask = torch.zeros(result_height, result_width_this_chunk, device=result.device)
        
        # å¯¹äºç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªåˆ†å—ï¼Œä½¿ç”¨å…¨æƒé‡
        if i == 0 or i == n_tiles - 1:
            chunk_mask[:, :] = 1.0
        else:
            # å¯¹äºä¸­é—´åˆ†å—ï¼Œåˆ›å»ºæ¸å˜æƒé‡
            for j in range(result_width_this_chunk):
                if j < image_overlap_size:
                    # å·¦ä¾§æ¸å˜ï¼šä»0åˆ°1
                    weight = j / image_overlap_size
                elif j > result_width_this_chunk - image_overlap_size:
                    # å³ä¾§æ¸å˜ï¼šä»1åˆ°0
                    weight = (result_width_this_chunk - j) / image_overlap_size
                else:
                    # ä¸­é—´éƒ¨åˆ†ï¼šå…¨æƒé‡
                    weight = 1.0
                
                chunk_mask[:, j] = weight
        
        # ç¡®ä¿åˆ†å—å®½åº¦ä¸éœ€è¦å®½åº¦åŒ¹é…
        if chunk_width != result_width_this_chunk:
            # ä½¿ç”¨æ’å€¼è°ƒæ•´åˆ†å—å°ºå¯¸
            image_chunk = torch.nn.functional.interpolate(
                image_chunk, 
                size=(result_height, result_width_this_chunk), 
                mode='bilinear', 
                align_corners=False
            )
            print(f"åˆ†å— {i}: ä½¿ç”¨æ’å€¼è°ƒæ•´å°ºå¯¸ä» {chunk_width} åˆ° {result_width_this_chunk}")
        
        # åº”ç”¨æƒé‡åˆ°åˆ†å—
        weighted_chunk = image_chunk * chunk_mask.unsqueeze(0).unsqueeze(0)
        
        # ç´¯åŠ åˆ°ç»“æœ
        result[:, :, :, start_pos:end_pos] += weighted_chunk
        
        # æ›´æ–°æ€»æƒé‡æ©ç çš„å¯¹åº”éƒ¨åˆ†
        blend_mask[:, start_pos:end_pos] += chunk_mask
    
    # é¿å…é™¤ä»¥é›¶
    blend_mask = torch.clamp(blend_mask, min=1e-8)
    
    # å½’ä¸€åŒ–ç»“æœ
    result = result / blend_mask.unsqueeze(0).unsqueeze(0)
    
    # æœ€ç»ˆå°ºå¯¸è°ƒæ•´ï¼ˆåº”è¯¥ä¸éœ€è¦ï¼Œä½†ä¿ç•™ä½œä¸ºä¿é™©ï¼‰
    if result.shape[3] != expected_width:
        result = torch.nn.functional.interpolate(
            result, 
            size=(result_height, expected_width), 
            mode='bilinear', 
            align_corners=False
        )
        print("ä½¿ç”¨æ’å€¼è¿›è¡Œæœ€ç»ˆå®½åº¦è°ƒæ•´")
    
    # æ¸…ç†å†…å­˜
    del images, blend_mask
    torch.cuda.empty_cache()
    
    return result

# æ£€æŸ¥å¹¶ç§»åŠ¨æ‰€æœ‰æ¨¡å‹å‚æ•°å’Œç¼“å†²åŒº
def move_model_to_cpu(model):
    model = model.to('cpu')
    
    # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨ CPU
    for param in model.parameters():
        param.data = param.data.cpu()
        if param.grad is not None:
            param.grad = param.grad.cpu()
    return model

import gc
import random
from wan23.utils.utils import best_output_size, masks_like

@torch.inference_mode()
def long_generate(g: LongGenArgs) -> Tuple[str, str]:
    if not WAN_READY or MODELS.wan_i2v is None or MODELS.vae is None or MODELS.transformer is None:
        raise RuntimeError("Wan æœªåŠ è½½ï¼Œè¯·å…ˆç‚¹å‡»â€œåŠ è½½æ‰€é€‰æ¨¡å‹â€ã€‚")

    os.makedirs(g.output_dir, exist_ok=True)
    device = MODELS.device
    transformer = MODELS.transformer
    vae = MODELS.vae
    wan = MODELS.wan_i2v

    print("long_generate", g.mode)
    is_i2v_mode = g.mode == "I2V"  # Check if in I2V mode
    is_t2v_mode = g.mode == "T2V"  # Check if in T2V mode

    # 3) é‡‡æ ·å¾ªç¯ï¼ˆå°¾éƒ¨ latent_frame_zero å¸§ï¼‰
    latent_frame_zero = int(g.latent_frame_zero)
    frame_zero = int(g.frame_zero)
    steps = int(g.sample_steps)
    sample_num = int(g.sample_num)
    shift = float(g.shift)
    frame_total = 0


    max_area=704*1280
    base_name = str(random.random())

    wan.text_encoder.model = wan.text_encoder.model.to("cpu")
    transformer = transformer.to("cpu")
    move_model_to_cpu(wan.text_encoder.model)
    move_model_to_cpu(transformer)
    torch.cuda.empty_cache()
    torch.cuda.empty_cache()
    gc.collect()


    # 1) åˆå§‹åŒ–
    if g.continue_from_last and LAST["last_model_input_de"] is not None:
        model_input_de: torch.Tensor = LAST["last_model_input_de"].to(device)               # (C,F,H,W)
        model_input_latent: torch.Tensor = LAST["last_model_input_latent"].to(device)       # (C,Fz,Hz,Wz)
        frame_total = int(LAST["frame_total"])
        first_img_path = None
    elif is_i2v_mode:
        if not g.jpg_path and is_i2v_mode:
            raise ValueError("é¦–è½®ç”Ÿæˆå¿…é¡»æä¾› jpg_pathï¼ˆå•å¼ å›¾ç‰‡è·¯å¾„ï¼‰ã€‚")
        
        pixel_values_vid, base_name, img_path = create_video_from_image(
                g.jpg_path, total_frames=frame_zero, H1=704, W1=1280
            )  # (F,C,H,W) 704 1280
            

        first_img_path = img_path
        pixel_values_vid = pixel_values_vid.permute(1,0,2,3).contiguous().to(device)  # (C,F,H,W)

        # å¤´éƒ¨å¤åˆ¶ 16 å¸§
        pixel_values_vid = torch.cat([pixel_values_vid[:,0:1].repeat(1,16,1,1),
                                      pixel_values_vid], dim=1)  # (C, 16+33, H, W)
        model_input_de = pixel_values_vid.clone()

        with torch.amp.autocast("cuda", dtype=DTYPE):
            lat_a = wan.vae.encode([model_input_de[:,:-frame_zero]])[0]
            lat_b = wan.vae.encode([model_input_de[:,-frame_zero:]])[0]
        model_input_latent = torch.cat([lat_a, lat_b], dim=1)  # (C,Fz,Hz,Wz)

        torch.cuda.empty_cache()
        torch.cuda.empty_cache()
        # vae.model = vae.model.to("cpu").to(DTYPE)
        # vae.scale[0] = vae.scale[0].to("cpu").to(DTYPE)
        # vae.scale[1] = vae.scale[1].to("cpu").to(DTYPE)
        # for p in vae.model.parameters():
        #     p.data = p.data.to("cpu").to(DTYPE)
        #print(latent_frame_zero,"latent_frame_zero")
        #with torch.amp.autocast("cuda", dtype=DTYPE):
            #print(vae.model.device,"W08RHF9W8HF9W8GHF9W48")
            #tiled_decode_overlap(wan.vae, model_input_latent[:,-latent_frame_zero:], latent_frame_zero=latent_frame_zero)
            #wan.vae.decode([model_input_latent[:,-latent_frame_zero:]])[0]
        
        print("vae_end")
        #zzzz

        frame_total = model_input_de.shape[1] - 16  # å¯è§†å¸§ï¼ˆæ‰£é™¤å¤´éƒ¨ 16ï¼‰

    # 2) Promptï¼ˆå¯é€‰å›¾ç‰‡ç²¾ç‚¼ï¼‰
    final_prompt = g.prompt
    if g.refine_from_image and first_img_path:
        final_prompt = refine_prompt_from_image(first_img_path, final_prompt)

    if g.caption_path:
        try:
            os.makedirs(os.path.dirname(g.caption_path), exist_ok=True)
            with open(g.caption_path, "w", encoding="utf-8") as f:
                f.write(final_prompt)
        except Exception as e:
            LOGGER.warning("write caption failed: %s", e)


    arg_c = {}; arg_null = {}; seq_len = None
    try:
        try:
            if hasattr(wan, "text_encoder") and hasattr(wan.text_encoder, "model"):
                wan.text_encoder.model = wan.text_encoder.model.to("cuda")
        except Exception:
            pass

        with torch.amp.autocast("cuda", dtype=DTYPE):
            if is_t2v_mode and not g.continue_from_last:
                gen_ret = wan.generate(
                    final_prompt,
                    frame_num=frame_zero,
                    max_area=max_area,
                    latent_frame_zero=latent_frame_zero,
                    sampling_steps=steps,
                    shift=shift,
                )
            else:
                if g.continue_from_last:
                    gen_ret = wan.generate(
                        final_prompt,
                        img=model_input_latent,
                        frame_num=model_input_de.shape[1]+frame_zero,
                        max_area=max_area,
                        latent_frame_zero=latent_frame_zero,
                        sampling_steps=steps,
                        shift=shift,
                    )
                else:
                    gen_ret = wan.generate(
                        final_prompt,
                        img=model_input_latent[:, :-latent_frame_zero],
                        frame_num=model_input_de.shape[1],
                        max_area=max_area,
                        latent_frame_zero=latent_frame_zero,
                        sampling_steps=steps,
                        shift=shift,
                    )
        try:
            if hasattr(wan, "text_encoder") and hasattr(wan.text_encoder, "model"):
                wan.text_encoder.model = wan.text_encoder.model.to("cpu")
            transformer = transformer.to("cuda")
        except Exception:
            pass


        if is_i2v_mode or g.continue_from_last:
            arg_c, arg_null, noise, mask2, img_lat = gen_ret
        else:
            arg_c, arg_null, noise = gen_ret
            
        if is_i2v_mode or g.continue_from_last:
            model_input_latent = _to_bf16(model_input_latent)

        noise = _to_bf16(noise)
        if is_i2v_mode:
            mask2     = _to_bf16(mask2)
            img_lat   = _to_bf16(img_lat)

        seq_len = int(arg_c.get("seq_len", 0))
        sampling_sigmas = get_sampling_sigmas(steps, shift)

        videos_to_concat = []
        if g.continue_from_last:
            videos_to_concat.append(model_input_de)

        for seg in range(sample_num):
            if seg == 0 and is_i2v_mode and not g.continue_from_last:
                latent = noise.clone()
                latent = _to_bf16(torch.cat([model_input_latent[:, :-latent_frame_zero, :, :], latent[:, -latent_frame_zero:, :, :]], dim=1))
            elif seg == 0 and is_t2v_mode and not g.continue_from_last:
                latent = noise.clone()
            else:
                latent = torch.randn(
                    wan.vae.model.z_dim, model_input_latent.shape[1] + latent_frame_zero,
                    model_input_latent.shape[2],
                    model_input_latent.shape[3],
                    dtype=DTYPE,
                    device=device
                )
                latent = _to_bf16(torch.cat([model_input_latent, latent[:, -latent_frame_zero:, :, :]], dim=1))
                mask1, mask2 = masks_like([latent], zero=True, latent_frame_zero=latent_frame_zero)

            #torch.randn_like(model_input_latent, dtype=DTYPE, device=device)

            #(1. - mask2[0]) * img_lat[0] + mask2[0] * latent)

            for i in range(steps):
                #ts_scalar = float(sampling_sigmas[i] * 1000.0)
                #tvec = torch.full((1, seq_len), ts_scalar, device=device, dtype=DTYPE)

                if is_i2v_mode or seg > 0 or g.continue_from_last:
                    ts_scalar = [sampling_sigmas[i]*1000]
                    timestep = torch.tensor(ts_scalar).to(device)
                    temp_ts = (mask2[0][0][:-latent_frame_zero, ::2, ::2] ).flatten()
                    temp_ts = torch.cat([
                                    temp_ts,
                                    temp_ts.new_ones(arg_c['seq_len'] - temp_ts.size(0)) * timestep
                                ])
                    tvec = temp_ts.unsqueeze(0)
                else:
                    ts_scalar = [sampling_sigmas[i]*1000]
                    timestep = torch.tensor(ts_scalar).to(device)
                    tvec = timestep

                latent_model_input = [_to_bf16(latent)]

                with torch.autocast("cuda", dtype=DTYPE):
                    if is_i2v_mode or seg > 0 or g.continue_from_last:
                        noise_pred = transformer(latent_model_input, t=tvec,latent_frame_zero = latent_frame_zero, **arg_c)[0]
                    else:
                        noise_pred = transformer(latent_model_input, t=tvec,latent_frame_zero = latent_frame_zero, **arg_c, flag=False)[0]

                tail = latent[:,-latent_frame_zero:,:,:]
                pred_tail = noise_pred[:,-latent_frame_zero:,:,:]
                if i+1 == steps:
                    new_tail = tail + (0.0 - sampling_sigmas[i]) * pred_tail
                else:
                    new_tail = tail + (sampling_sigmas[i+1] - sampling_sigmas[i]) * pred_tail
                new_tail = _to_bf16(new_tail)
                latent = _to_bf16(torch.cat([latent[:,:-latent_frame_zero,:,:], new_tail], dim=1))

            transformer = transformer.to("cpu")
            move_model_to_cpu(transformer)
            torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            gc.collect()


            with torch.amp.autocast("cuda", dtype=DTYPE):
                #video_tail = vae.decode([latent[:,-latent_frame_zero:]])[0].cuda() 
                video_tail = tiled_decode_overlap(wan.vae, latent, latent_frame_zero=latent_frame_zero) #vae.decode([latent[:,-latent_frame_zero:]])[0]

            videos_to_concat.append(video_tail)
            
            if is_i2v_mode or seg > 0:
                model_input_latent = torch.cat([model_input_latent[:,:-latent_frame_zero,:,:],
                                                latent[:,-latent_frame_zero:,:,:]], dim=1)
            else:
                model_input_latent = latent[:,-latent_frame_zero:,:,:]

            with torch.amp.autocast("cuda", dtype=DTYPE):
                #video_tail_px = vae.decode([latent[:,-latent_frame_zero:]])[0].cuda() 
                video_tail_px = video_tail #tiled_decode_overlap(wan.vae, latent, latent_frame_zero=latent_frame_zero) # vae.decode([latent[:,-latent_frame_zero:]])[0]
           
            transformer = transformer.to("cuda")


            # if video_tail_px.shape[1] < frame_zero:
            #     pad = video_tail_px[:,0:1,:,:].repeat(1, frame_zero - video_tail_px.shape[1], 1, 1)
            #     video_tail_px = torch.cat([pad, video_tail_px], dim=1)
            if is_i2v_mode or seg > 0:
                model_input_de = torch.cat([model_input_de[:,:-frame_zero,:,:],
                                            video_tail_px[:,-frame_zero:,:,:]], dim=1)
            else:
                model_input_de = video_tail_px[:,-frame_zero:,:,:]

            frame_total +=  video_tail_px[:,-frame_zero:,:,:].shape[1] #frame_zero

    

        video_cat = torch.cat(videos_to_concat, dim=1)
        ts = int(time.time())
        out_path = os.path.join(g.output_dir, f"{ts}_long.mp4")
        _postprocess_video(video_cat, g.fps, out_path)

        LAST["last_model_input_latent"] = model_input_latent.detach()#.to("cpu")
        LAST["last_model_input_de"] = model_input_de.detach()#.to("cpu")
        LAST["frame_total"] = frame_total
        LAST["last_video_path"] = out_path
        LAST["last_prompt"] = final_prompt

        return out_path, final_prompt
    finally:
        None


# ============================= Flask App ================================
app = Flask(__name__, static_url_path="/outputs", static_folder=OUTPUT_DIR)
if _HAS_CORS:
    CORS(app)

# ---- Home page (simple UI) ----
_HTML = """
<!DOCTYPE html>
<html lang="zh">
<head>
<meta charset="utf-8"/>
<title>Long Video Generation (Flask, BF16, Single-GPU)</title>
<style>
:root {
  --bg:#0b1021; --fg:#c8d3f5; --muted:#8a98c9; --ok:#2ecc71; --err:#ff6b6b; --panel:#12183a; --accent:#7aa2f7;
}
* { box-sizing: border-box; }
body { font-family: ui-sans-serif, system-ui, Segoe UI, Arial; margin:0; color:var(--fg); background:linear-gradient(120deg,#0b1021,#10173a); }
header { padding:18px 28px; background:rgba(0,0,0,.25); position:sticky; top:0; backdrop-filter: blur(8px); border-bottom:1px solid #1e2754; }
h1 { margin:0; font-size:20px; letter-spacing:.4px; }
main { padding:24px; max-width:1080px; margin:0 auto; }
.card { background:var(--panel); border:1px solid #1b2450; border-radius:16px; padding:16px 18px; margin-bottom:16px; box-shadow:0 10px 30px rgba(0,0,0,.25); }
.row { display:flex; gap:16px; flex-wrap:wrap; }
.col { flex:1 1 340px; min-width:320px; }
label { display:block; margin:8px 0 6px; color:#aab6ee; font-size:13px; }
input[type=text], input[type=number], textarea {
  width:100%; padding:10px 12px; border-radius:12px; border:1px solid #27306a; background:#0d1433; color:var(--fg);
}
textarea { min-height:120px; }
button { padding:10px 16px; border-radius:12px; border:1px solid #2b336d; background:linear-gradient(180deg,#172154,#101a46);
  color:#e9edff; cursor:pointer; transition: transform .05s ease, box-shadow .2s;
}
button:hover { box-shadow:0 8px 18px rgba(0,0,0,.35); }
button:active { transform: translateY(1px) scale(.99); }
.badge { display:inline-flex; align-items:center; gap:8px; padding:6px 10px; border-radius:999px; border:1px solid #2a3168; background:#0e1540; margin-right:8px; font-size:12px; color:#b7c3ff; }
.badge.ok { background: rgba(46,204,113,.1); border-color:#284b36; color:#80ffb3; }
.badge.err { background: rgba(255,107,107,.1); border-color:#5a2a2a; color:#ff9b9b; }
video { width:100%; max-height:420px; outline: 1px solid #1e2754; border-radius:12px; background:#000; }
pre { margin:0; white-space:pre-wrap; word-break:break-word; }
.panel-title { font-weight:600; color:#bcd1ff; margin-bottom:6px; }
#overlay {
  position: fixed; inset: 0; background: rgba(10, 14, 35, .66);
  display:none; align-items: center; justify-content: center; backdrop-filter: blur(2px); z-index:999;
}
.spinner {
  width:56px; height:56px; border-radius:50%; border:4px solid rgba(255,255,255,.18);
  border-top-color: var(--accent); animation: spin 1s linear infinite;
}
@keyframes spin { to { transform: rotate(360deg) } }
.small { font-size:12px; color:var(--muted); }
</style>
</head>
<body>
<header>
  <h1>ğŸ“¹ é•¿è§†é¢‘ç”Ÿæˆ â€” Flask / BF16 / å•å¡</h1>
</header>
<div id="overlay"><div class="spinner"></div></div>
<main>

<div class="card">
  <div>
    <span id="wan_state" class="badge">Wan: æœªåŠ è½½</span>
    <span id="cap_state" class="badge">InternVL: æœªåŠ è½½</span>
  </div>
  <div style="margin-top:10px; display:flex; gap:8px; flex-wrap:wrap;">
    <label><input id="chk_wan" type="checkbox"/> åŠ è½½ Wan (DiT + VAE + T5)</label>
    <label><input id="chk_cap" type="checkbox"/> åŠ è½½ InternVL (Caption)</label>
    <button onclick="doLoad()">ğŸ“¦ åŠ è½½æ‰€é€‰</button>
  </div>
</div>

<div class="card row">
  <div class="col">
    <div class="panel-title">1) æ¡ä»¶ä¸å‚æ•°</div>

    <!-- Mode Selection -->
    <div style="margin-top:8px;">
      <label><input id="mode_i2v" type="radio" name="mode" checked /> I2V æ¨¡å¼</label>
      <label><input id="mode_t2v" type="radio" name="mode" /> T2V æ¨¡å¼</label>
      <br/>
    </div>

    <label>é¦–å¸§å›¾ç‰‡è·¯å¾„ (jpg_path)</label>
    <input id="jpg_path" type="text" placeholder="ä¾‹å¦‚ D:\\imgs\\001.jpg"/>

    <label>Prompt</label>
    <textarea id="prompt" rows="5">A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage...</textarea>

    <div class="row">
      <div class="col">
        <label>FPS</label>
        <input id="fps" type="number" value="15"/>
      </div>
      <div class="col">
        <label>é‡‡æ ·æ­¥æ•° (steps)</label>
        <input id="steps" type="number" value="50"/>
      </div>
      <div class="col">
        <label>æ¯æ®µå¯è§†å¸§ (frame_zero)</label>
        <input id="frame_zero" type="number" value="32"/>
      </div>
    </div>

    <div class="row">
      <div class="col">
        <label>å°¾éƒ¨æ½œå¸§ (latent_frame_zero)</label>
        <input id="latent_zero" type="number" value="8"/>
      </div>
      <div class="col">
        <label>æ®µæ•° (sample_num)</label>
        <input id="sample_num" type="number" value="1"/>
      </div>
      <div class="col">
        <label>shift</label>
        <input id="shift" type="number" step="0.1" value="7.0"/>
      </div>
    </div>

    <div class="row">
      <div class="col">
        <label>Seedï¼ˆ-1 éšæœºï¼‰</label>
        <input id="seed" type="number" value="-1"/>
      </div>
      <div class="col">
        <label>è¾“å‡ºç›®å½•</label>
        <input id="out_dir" type="text" value="outputs"/>
      </div>
      <div class="col">
        <label>(å¯é€‰) ä¿å­˜ç²¾ç‚¼æ–‡æ¡ˆåˆ°æ–‡ä»¶</label>
        <input id="cap_path" type="text"/>
      </div>
    </div>

    <div style="margin-top:8px;">
      <label><input id="cont" type="checkbox"/> ç»§ç»­ç»­å¸§ï¼ˆä½¿ç”¨ä¸Šæ¬¡ç”Ÿæˆçš„å¹²å‡€ latent ä½œä¸ºæ¡ä»¶ï¼‰</label>
      <br/>
      <label><input id="refine" type="checkbox"/> ä»å›¾ç‰‡ç²¾ç‚¼ Promptï¼ˆéœ€åŠ è½½ InternVLï¼‰</label>
    </div>

    <div style="margin-top:12px;">
      <button onclick="doGen()">ğŸš€ å¼€å§‹/ç»§ç»­ ç”Ÿæˆ</button>
    </div>
  </div>

  <div class="col">
    <div class="panel-title">2) é¢„è§ˆ</div>
    <video id="video" controls></video>
    <div class="small" style="margin-top:6px;">ç”ŸæˆæˆåŠŸåä¼šè‡ªåŠ¨æ›´æ–°åˆ°æœ€æ–°ã€‚</div>
  </div>
</div>

<div class="card">
  <div class="panel-title">æ—¥å¿—</div>
  <pre id="log">ï¼ˆç‚¹å‡»â€œæ‹‰å–æ—¥å¿—â€æŸ¥çœ‹ï¼‰</pre>
  <div style="margin-top:8px;">
    <button onclick="pullLog()">æ‹‰å–æ—¥å¿—</button>
  </div>
</div>

<div class="card">
  <div class="panel-title">é”™è¯¯è¯¦æƒ…ï¼ˆå®Œæ•´ traceback ï¼‰</div>
  <pre id="trace">ï¼ˆè‹¥å‘ç”Ÿé”™è¯¯ï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºå®Œæ•´å †æ ˆï¼‰</pre>
</div>

</main>

<script>
let LOG_PATH = "";

function el(id) { return document.getElementById(id); }
function showOverlay(v) {
  el('overlay').style.display = v ? 'flex' : 'none';
}
async function refreshStatus() {
  const r = await fetch('/api/status'); const j = await r.json();
  const wan = el('wan_state'); const cap = el('cap_state');
  wan.textContent = 'Wan: ' + (j.wan_ready ? 'å·²åŠ è½½' : 'æœªåŠ è½½');
  cap.textContent = 'InternVL: ' + (j.cap_ready ? 'å·²åŠ è½½' : 'æœªåŠ è½½');
  wan.className = 'badge ' + (j.wan_ready ? 'ok' : '');
  cap.className = 'badge ' + (j.cap_ready ? 'ok' : '');
  LOG_PATH = j.log_path || '';
}
async function doLoad() {
  el('trace').textContent = '';
  showOverlay(true);
  try {
    const sel = { wan: el('chk_wan').checked, cap: el('chk_cap').checked };
    const r = await fetch('/api/load', {
      method:'POST', headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(sel)
    });
    const j = await r.json();
    let log = '';
    if (j.wan_msg) log += j.wan_msg + '\\n';
    if (j.cap_msg) log += j.cap_msg + '\\n';
    if (LOG_PATH) log += 'æ—¥å¿—æ–‡ä»¶: ' + LOG_PATH;
    el('log').textContent = log;
    if (!j.success) el('trace').textContent = j.trace || '';
  } catch (e) {
    el('trace').textContent = String(e);
  } finally {
    showOverlay(false);
    await refreshStatus();
  }
}
async function doGen() {
  el('trace').textContent = '';
  showOverlay(true);
  try {
    const payload = {
      prompt: el('prompt').value,
      jpg_path: el('jpg_path').value,
      output_dir: el('out_dir').value,
      fps: parseInt(el('fps').value||'15'),
      sample_steps: parseInt(el('steps').value||'50'),
      sample_num: parseInt(el('sample_num').value||'1'),
      frame_zero: parseInt(el('frame_zero').value||'32'),
      latent_frame_zero: parseInt(el('latent_zero').value||'8'),
      shift: parseFloat(el('shift').value||'7'),
      seed: parseInt(el('seed').value||'-1'),
      continue_from_last: el('cont').checked,
      refine_from_image: el('refine').checked,
      caption_path: el('cap_path').value,
      mode: el('mode_t2v').checked ? "T2V" : "I2V",  // This line ensures correct mode
    };
    const r = await fetch('/api/generate_long', {
      method:'POST', headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload)
    });
    const j = await r.json();
    if (j.success) {
      if (j.video_rel) el('video').src = j.video_rel;
      let msg = (j.info || '');
      if (LOG_PATH) msg += '\\næ—¥å¿—æ–‡ä»¶: ' + LOG_PATH;
      el('log').textContent = msg;
    } else {
      let msg = (j.error || 'ERROR');
      if (LOG_PATH) msg += '\\næ—¥å¿—æ–‡ä»¶: ' + LOG_PATH;
      el('log').textContent = msg;
      el('trace').textContent = j.trace || '';
    }
  } catch (e) {
    el('trace').textContent = String(e);
  } finally {
    showOverlay(false);
  }
}
async function pullLog() {
  const r = await fetch('/api/log/tail?n=500');
  const t = await r.text();
  el('log').textContent = t;
}
refreshStatus();
</script>
</body>
</html>
"""

@app.route("/")
def home():
    return Response(_HTML, mimetype="text/html; charset=utf-8")

@app.get("/api/status")
def api_status():
    return jsonify({
        "wan_ready": bool(WAN_READY),
        "cap_ready": bool(CAP_READY),
        "log_path": LOG_PATH,
        "last_video": LAST.get("last_video_path"),
        "frame_total": LAST.get("frame_total", 0),
    })

@app.post("/api/load")
def api_load():
    data = request.get_json(force=True, silent=True) or {}
    to_load_wan = bool(data.get("wan"))
    to_load_cap = bool(data.get("cap"))
    rst = {"success": True, "wan_msg": None, "cap_msg": None, "trace": None}

    try:
        if to_load_wan:
            rst["wan_msg"] = load_wan()
    except Exception as e:
        LOGGER.exception("[api_load] load_wan failed: %s", e)
        rst["success"] = False
        rst["wan_msg"] = f"[ERROR@Wan] {type(e).__name__}: {e}"
        rst["trace"] = _trace_text(e)

    try:
        if to_load_cap:
            rst["cap_msg"] = load_caption_model()
    except Exception as e:
        LOGGER.exception("[api_load] load_caption failed: %s", e)
        rst["success"] = False
        rst["cap_msg"] = f"[ERROR@InternVL] {type(e).__name__}: {e}"
        rst["trace"] = (rst["trace"] or "") + "\n\n" + _trace_text(e)

    return jsonify(rst)

@app.post("/api/generate_long")
def api_generate_long():
    data = request.get_json(force=True, silent=True) or {}
    try:
        g = LongGenArgs(
            prompt=str(data.get("prompt") or ""),
            jpg_path=(str(data.get("jpg_path") or "") or None),
            output_dir=str(data.get("output_dir") or OUTPUT_DIR),
            fps=int(data.get("fps") or 15),
            sample_steps=int(data.get("sample_steps") or 50),
            sample_num=int(data.get("sample_num") or 1),
            frame_zero=int(data.get("frame_zero") or 32),
            latent_frame_zero=int(data.get("latent_frame_zero") or 8),
            shift=float(data.get("shift") or 7.0),
            seed=int(data.get("seed") or -1),
            continue_from_last=bool(data.get("continue_from_last")),
            refine_from_image=bool(data.get("refine_from_image")),
            caption_path=(str(data.get("caption_path") or "") or None),
            mode=str(data.get("mode") or "I2V"),  # Added mode parameter
        )
        print(g.mode, "g_mode")
        # Check only for I2V mode if jpg_path is provided when not continuing from the last frame
        if g.mode == "I2V" and (not g.continue_from_last and not g.jpg_path):
            raise ValueError("é¦–è½®ç”Ÿæˆå¿…é¡»æä¾› jpg_pathï¼ˆå•å¼ å›¾ç‰‡è·¯å¾„ï¼‰ã€‚è‹¥è¦ç»­å¸§ï¼Œè¯·å‹¾é€‰â€œç»§ç»­ç»­å¸§â€ã€‚")

        out_path, final_prompt = long_generate(g)
        out_abs = os.path.abspath(out_path)
        rel = os.path.relpath(out_abs, OUTPUT_DIR).replace("\\", "/")
        video_rel = f"/outputs/{rel}"

        return jsonify({
            "success": True,
            "video_abs": out_abs,
            "video_rel": video_rel,
            "info": f"Saved to {out_abs} | Device cuda:{DEVICE_ID} | DType BF16",
            "prompt": final_prompt
        })
    except Exception as e:
        LOGGER.exception("[api_generate_long] failed: %s", e)
        return jsonify({
            "success": False,
            "error": f"{type(e).__name__}: {e}",
            "trace": _trace_text(e),
        })

@app.get("/api/log/tail")
def api_log_tail():
    n = int(request.args.get("n", 200))
    try:
        with open(LOG_PATH, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
        tail = "".join(lines[-n:])
        return Response(tail, mimetype="text/plain; charset=utf-8")
    except Exception as e:
        return Response(f"[log read error] {e}", mimetype="text/plain; charset=utf-8")

@app.get("/outputs/<path:filename>")
def static_outputs(filename):
    return send_from_directory(OUTPUT_DIR, filename, as_attachment=False)

# ------------------------------- Launcher --------------------------------
def _port_is_free(h, p) -> bool:
    s = socket.socket(); s.settimeout(0.2)
    ok = s.connect_ex((h, p)) != 0
    s.close()
    return ok

def main():
    os.environ.setdefault("HF_HOME", os.path.abspath(".cache/huggingface"))

    host = "127.0.0.1"
    port = int(os.environ.get("WEB_PORT", "7666"))

    if not _port_is_free(host, port):
        LOGGER.error("[PORT] ç«¯å£å·²è¢«å ç”¨ï¼š%s:%s", host, port)
        LOGGER.error("       è§£å†³ï¼šå…³é—­å ç”¨ç«¯å£çš„ç¨‹åºï¼Œæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ WEB_PORT æ”¹ç«¯å£")
        print("Press Enter to exit â€¦"); 
        try: input()
        except Exception: pass
        return

    url = f"http://{host}:{port}"
    LOGGER.info("[LAUNCH] å³å°†å¯åŠ¨ï¼š%s", url)
    LOGGER.info("[LAUNCH] Log file: %s", LOG_PATH)

    try:
        app.run(host=host, port=port, debug=False, threaded=True)
    except Exception as e:
        LOGGER.critical("[LAUNCH] å¯åŠ¨å¤±è´¥ï¼š%s", e, exc_info=True)
        print("Press Enter to exit â€¦"); 
        try: input()
        except Exception: pass

if __name__ == "__main__":
    main() ,éœ€è¦åš3ä¸ªä¿®æ”¹ï¼Œç¬¬ä¸€ä¸ªä¿®æ”¹ç‚¹åŠ å…¥æŒ‰é’®æ§åˆ¶promptï¼Œ        vocab1 = { 
            "W": "The camera pushes forward (W).",
            "A": "The camera moves to the left (A).",
            "S": "The camera pulls back (S).",
            "D": "The camera moves to the right (D).",
            "W+A": "The camera pushes forward and moves to the left (W+A).",
            """: "The camera pushes forward and moves to the right (W+D).", 
            "S+D": "The camera pulls back and moves to the right (S+D).",
            "S+A": "The camera pulls back and moves to the left (S+A).",
            "None": "The camera's movement direction remains stationary (Â·).",
        }ï¼Œè¿™æœ‰5ä¸ªæŒ‰é’®åŒ…æ‹¬äº†W,S,A,Dï¼Œ.(ä»£è¡¨None), åˆ†åˆ«ä»¥ä¸Šä¸‹å·¦å³ä¸­é—´æ’åˆ—ï¼Œ 
        vocab2 = { 
            "â†’": "The camera pans to the right (â†’).",
            "â†": "The camera pans to the left (â†).",
            "â†‘": "The camera tilts up (â†‘).",
            "â†“": "The camera tilts down (â†“).",
            "â†‘â†’": "The camera tilts up and pans to the right (â†‘â†’).",
            "â†‘â†": "The camera tilts up and pans to the left (â†‘â†).",
            "â†“â†’": "The camera tilts down and pans to the right (â†“â†’).",
            "â†“â†": "The camera tilts down and pans to the left (â†“â†).",
            "Â·": "The rotation direction of the camera remains stationary (Â·)."
        }ï¼Œè¿™ä¹Ÿæœ‰5ä¸ªæŒ‰é’®åŒ…æ‹¬äº†â†‘,â†“,â†,â†’ï¼Œ.(ä»£è¡¨None), åˆ†åˆ«ä»¥ä¸Šä¸‹å·¦å³ä¸­é—´æ’åˆ—ã€‚è¿™äº›ä½ç½®æ”¾åœ¨1) æ¡ä»¶ä¸å‚æ•°ä¸­çš„Promptä¸Šé¢ï¼Œæ’åˆ—è¿™äº›é”®ç›˜ä¸€å®šè¦çœ‹èµ·æ¥ç¾è§‚èˆ’é€‚ã€‚æ³¨æ„ç»„åˆåªæœ‰"W+A","W+D"ï¼Œ "â†“â†"ç­‰ç­‰vocab1 å’Œvocab2å‡ºç°çš„"None"ä¸èƒ½å’Œwasdç»„åˆï¼Œ "Â·"ä¹Ÿä¸€æ ·ã€‚å°†è¿™äº›æŒ‰é’®æ§åˆ¶å¾—åˆ°åçš„promptå’ŒåŸå§‹ç”¨æˆ·è¾“å…¥çš„g.promptè¿æ¥ä¸€èµ·ä½œä¸ºæ–°çš„g.promptã€‚å¯¹äºç¬¬2ç‚¹ä¿®æ”¹ï¼ŒåŠ å…¥ä¸­è‹±æ–‡æŒ‰é’®è½¬æ¢ï¼Œå°†å‰ç«¯ç•Œé¢è½¬æ¢æˆè‹±æ–‡æˆ–è€…ä¸­æ–‡ï¼ŒæŒ‰é’®åœ¨æœ€ä¸Šé¢ã€‚å¯¹äºç¬¬3ç‚¹ä¿®æ”¹ï¼ŒåŠ å…¥åˆ†è¾¨ç‡ä¿®æ”¹ï¼Œå¯ä»¥é€‰æ‹©544x960æˆ–è€…704x1280ã€‚ç§»é™¤å°¾éƒ¨æ½œå¸§ (latent_frame_zero)ï¼Œå› ä¸ºlatent_frame_zero = (frame_zero-1)//4+1,ç”¨è¿™ä¸ªä½œä¸ºlatent_frame_zeroã€‚åŠ å…¥æ˜¾å­˜å ç”¨ä¼˜åŒ–æŒ‰é’®ï¼Œå¦‚æœä½¿ç”¨ï¼Œåˆ™"    wan.text_encoder.model = wan.text_encoder.model.to("cpu") 
    transformer = transformer.to("cpu")
    move_model_to_cpu(wan.text_encoder.model)
    move_model_to_cpu(transformer)
    torch.cuda.empty_cache()
    torch.cuda.empty_cache()
    gc.collect()
"è¿™æ®µåœ¨long_generateå‡½æ•°çš„ä»£ç æ˜¯è¿è¡Œçš„ï¼Œå¦åˆ™ä¸è¿è¡Œï¼Œæ·»åŠ æç¤º(ä½¿ç”¨è¿™ä¸ªé™ä½æ˜¾å­˜å ç”¨ï¼Œä½†æ˜¯å‡å°‘ç”Ÿæˆé€Ÿåº¦)ï¼ŒåŠ å…¥VAEæ˜¾å­˜å ç”¨ä¼˜åŒ–æŒ‰é’®ï¼Œå¦‚æœä½¿ç”¨ï¼Œåˆ™video_tail = tiled_decode_overlap(wan.vae, latent, latent_frame_zero=latent_frame_zero)ï¼Œå¦åˆ™video_tail = vae.decode([latent[:,-latent_frame_zero:]])[0]ï¼Œæ·»åŠ æç¤º(ä½¿ç”¨è¿™ä¸ªé™ä½æ˜¾å­˜å ç”¨ï¼Œä½†æ˜¯å¯èƒ½ä¼šå½±å“ç”Ÿæˆè´¨é‡)ã€‚æ³¨æ„åªä¿®æ”¹æˆ‘æåˆ°çš„éƒ¨åˆ†ï¼Œå…¶ä»–éƒ¨åˆ†ä¸ç®¡å†™çš„æ€æ ·éƒ½ä¸è¦ä¿®æ”¹ï¼Œä¸€å­—ä¸€å¥éƒ½ä¸è¦ä¿®æ”¹ï¼ŒæŠŠå®Œæ•´çš„ä»£ç å‘ç»™æˆ‘ï¼Œåƒä¸‡æ³¨æ„ä¸è¦ä¿®æ”¹å…¶ä»–éƒ¨åˆ†: